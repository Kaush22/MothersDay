{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nl\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"train.csv\")\n",
    "XX = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id  retweet_count  sentiment_class\n",
      "count  3.235000e+03    3235.000000      3235.000000\n",
      "mean   1.246352e+18       0.708501        -0.001236\n",
      "std    4.814373e+15       4.127832         0.688719\n",
      "min    1.240000e+18       0.000000        -1.000000\n",
      "25%    1.240000e+18       0.000000         0.000000\n",
      "50%    1.250000e+18       0.000000         0.000000\n",
      "75%    1.250000e+18       0.000000         0.000000\n",
      "max    1.250000e+18     118.000000         1.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3235 entries, 0 to 3234\n",
      "Data columns (total 6 columns):\n",
      "id                 3235 non-null float64\n",
      "original_text      3235 non-null object\n",
      "lang               3231 non-null object\n",
      "retweet_count      3235 non-null int64\n",
      "original_author    3235 non-null object\n",
      "sentiment_class    3235 non-null int64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 151.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train = X.copy()\n",
    "data_test = XX.copy()\n",
    "print(X.describe())\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 de\n",
      "683 ar\n",
      "974 fr\n",
      "1022 sl\n",
      "1238 ja\n",
      "1301 ar\n",
      "1328 ar\n",
      "1530 so\n",
      "1866 tl\n",
      "1943 ar\n",
      "2207 ar\n",
      "2400 ar\n",
      "2647 ar\n",
      "2650 it\n",
      "2984 de\n",
      "3006 ar\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(lang)):\n",
    "#     if(lang[i]!='en'):\n",
    "#         print(i, lang[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.lang = [detect(wd) for wd in data_train.original_text]\n",
    "data_test.lang = [detect(wd1) for wd1 in data_test.original_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang1 = [detect(wd) for wd in data_test.original_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( [(i, data_train.lang[i]) for i in range(len(data_train.lang)) if data_train.lang[i] != 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print([data_train.drop(i, axis = 0, inplace=True) for i in range(len(data_train.lang)) if data_train.lang[i] != 'en'])\n",
    "print([data_test.drop(i, axis = 0, inplace=True) for i in range(len(data_test.id)) if data_test.lang[i] != 'en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Kaush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "nl.download('words')\n",
    "words = set(nl.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(txt):\n",
    "    txt = txt.lower()\n",
    "    txt_l = re.sub('(\\â)|(\\€™)|(€)','',txt)\n",
    "    txt_l = re.sub(\"(#)[a-zA-Z0-9]\", \" \", txt_l)\n",
    "    txt_l = re.sub(r\"(twitter.com).*?[a-zA-Z0-9].*\\s\", \" \", txt_l)\n",
    "    txt_l = re.sub(r\"(https://).*(.com).*/([a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))\", \" \", txt_l)\n",
    "    txt_l = \" \".join(w for w in nl.wordpunct_tokenize(txt_l) if w.lower() in words or not w.isalpha())\n",
    "    txt_ = nlp(txt_l)\n",
    "    txt1 = [wd.text for wd in txt_ if wd.is_stop != True and wd.pos_ != 'PUNCT' ]\n",
    "    txt1 = \" \".join(txt1)\n",
    "    txt1 = re.sub(\"[$-_@.&+]\", \"\", txt1)\n",
    "    txt1 = re.sub(\"\\s+\", \" \", txt1)\n",
    "    \n",
    "    return(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.original_text = data_train.original_text.apply(processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_train.original_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               idx                                      original_text\n",
      "0     1.250000e+18  happy amazing know s hard able today s protect...\n",
      "1     1.250000e+18  happy day mum m sorry t bring day honestly poi...\n",
      "2     1.250000e+18  happy day days work today quiet time reflect d...\n",
      "3     1.240000e+18  happy day beautiful woman royalty mummy emeral...\n",
      "4     1.240000e+18   amazing ladies late grandmother iris mum caro...\n",
      "...            ...                                                ...\n",
      "3230  1.250000e+18  law wishing happy day praising day glory lord ...\n",
      "3231  1.240000e+18  happy mother s day step amazing othersday cwhu...\n",
      "3232  1.250000e+18  happy day woman know thanks pushing best perso...\n",
      "3233  1.240000e+18  happy mother s day amazing wife love like craz...\n",
      "3234  1.250000e+18  wishing safe happy day ferry inn coijcdg wkouv...\n",
      "\n",
      "[3219 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(columns = ['idx', 'original_text'])\n",
    "data['idx'] = data_train.id.copy()\n",
    "data['original_text'] = data_train.original_text.copy()\n",
    "print(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_train.sentiment_class, test_size = 0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1,2), stop_words = STOP_WORDS, lowercase = True)\n",
    "tf.fit_transform(data_train.original_text)\n",
    "train_X = tf.transform(X_train.original_text)\n",
    "test_X = tf.transform(X_test.original_text)\n",
    "type(train_X)\n",
    "# k = pd.DataFrame(X_train)\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>(0, 28024)\\t0.39573253363137806\\n  (0, 28007...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               idx                                      original_text\n",
       "2376  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "16    1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "1765  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "2730  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "1500  1.250000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "...            ...                                                ...\n",
       "3211  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "1351  1.250000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "528   1.250000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "3213  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "1294  1.240000e+18    (0, 28024)\\t0.39573253363137806\\n  (0, 28007...\n",
       "\n",
       "[2575 rows x 2 columns]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X = X_train.copy()\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.25e+18,\n",
       "        'happy amazing know s hard able today s protect vulnerable society pic twitter vanfjfqb',\n",
       "        'en', 0, 'BeenXXPired', 0],\n",
       "       [1.25e+18,\n",
       "        'happy day mum m sorry t bring day honestly point d walk hot able soon love lots p s need goo mvxblrsczdte by',\n",
       "        'en', 1, 'FestiveFeeling', 0],\n",
       "       [1.25e+18,\n",
       "        'happy day days work today quiet time reflect dog walk finish garden learn guitar drunk strawberry gin tonic watch lee evens place visit solate pic twitter gzxvvff',\n",
       "        'en', 0, 'KrisAllenSak', -1],\n",
       "       ...,\n",
       "       [1.25e+18,\n",
       "        'happy day woman know thanks pushing best person cqebsk', 'en',\n",
       "        0, 'localcreativity', 0],\n",
       "       [1.24e+18,\n",
       "        'happy mother s day amazing wife love like crazy bit weird nxdbnpi',\n",
       "        'en', 0, 'LoveluK77651882', 0],\n",
       "       [1.25e+18, 'wishing safe happy day ferry inn coijcdg wkouvsqkt',\n",
       "        'en', 0, 'andreaanderegg', -1]], dtype=object)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.original_text = X_train.original_text.values\n",
    "X_test.original_text = X_test.original_text.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aamavsl',\n",
       " 'aaubfsl',\n",
       " 'abandoned',\n",
       " 'abandoned know',\n",
       " 'abandoned youth',\n",
       " 'abblhtfr',\n",
       " 'abiding',\n",
       " 'abiding positively',\n",
       " 'able',\n",
       " 'able able',\n",
       " 'able amazing',\n",
       " 'able better',\n",
       " 'able celebrate',\n",
       " 'able choice',\n",
       " 'able come',\n",
       " 'able cuddle',\n",
       " 'able daughter',\n",
       " 'able day',\n",
       " 'able enjoy',\n",
       " 'able experience',\n",
       " 'able find',\n",
       " 'able fly',\n",
       " 'able fresh',\n",
       " 'able great',\n",
       " 'able happy',\n",
       " 'able hug',\n",
       " 'able inside',\n",
       " 'able join',\n",
       " 'able juggle',\n",
       " 'able kiss',\n",
       " 'able laugh',\n",
       " 'able let',\n",
       " 'able like',\n",
       " 'able live',\n",
       " 'able look',\n",
       " 'able mind',\n",
       " 'able mum',\n",
       " 'able normal',\n",
       " 'able person',\n",
       " 'able physically',\n",
       " 'able pic',\n",
       " 'able properly',\n",
       " 'able ran',\n",
       " 'able scattered',\n",
       " 'able send',\n",
       " 'able share',\n",
       " 'able soon',\n",
       " 'able sooner',\n",
       " 'able spend',\n",
       " 'able throw',\n",
       " 'able time',\n",
       " 'able today',\n",
       " 'able video',\n",
       " 'able visit',\n",
       " 'able watch',\n",
       " 'able wonderful',\n",
       " 'abode',\n",
       " 'abode like',\n",
       " 'abort',\n",
       " 'abort brought',\n",
       " 'aborted',\n",
       " 'aborted god',\n",
       " 'abortion',\n",
       " 'abortion happy',\n",
       " 'abpruc',\n",
       " 'abqmpsvnpus',\n",
       " 'abroad',\n",
       " 'abroad beautiful',\n",
       " 'abroad quarantine',\n",
       " 'abrupt',\n",
       " 'abrupt end',\n",
       " 'absent',\n",
       " 'absent pic',\n",
       " 'absent try',\n",
       " 'absolute',\n",
       " 'absolute angel',\n",
       " 'absolute babe',\n",
       " 'absolute belter',\n",
       " 'absolute cracker',\n",
       " 'absolute cutie',\n",
       " 'absolute dream',\n",
       " 'absolute gem',\n",
       " 'absolute hero',\n",
       " 'absolute inspiration',\n",
       " 'absolute legend',\n",
       " 'absolute love',\n",
       " 'absolute noa',\n",
       " 'absolute queen',\n",
       " 'absolute rock',\n",
       " 'absolute superhero',\n",
       " 'absolute sure',\n",
       " 'absolute thanks',\n",
       " 'absolutely',\n",
       " 'absolutely amazing',\n",
       " 'absolutely beautiful',\n",
       " 'absolutely correct',\n",
       " 'absolutely crazy',\n",
       " 'absolutely necessary',\n",
       " 'absolutely remarkable',\n",
       " 'absolutely right',\n",
       " 'absolutely smashing',\n",
       " 'absolutely social',\n",
       " 'absqlhlk',\n",
       " 'abundance',\n",
       " 'abundance happy',\n",
       " 'abundantly',\n",
       " 'abundantly appymothersday',\n",
       " 'abuse',\n",
       " 'abuse pic',\n",
       " 'abuse selfish',\n",
       " 'abuser',\n",
       " 'abuser known',\n",
       " 'aby',\n",
       " 'aby amma',\n",
       " 'aby ife',\n",
       " 'aby myzbstpjz',\n",
       " 'aby omen',\n",
       " 'aby pic',\n",
       " 'aby xwphcoq',\n",
       " 'academy',\n",
       " 'academy happy',\n",
       " 'academy million',\n",
       " 'accept',\n",
       " 'accept loving',\n",
       " 'accept sincere',\n",
       " 'acceptable',\n",
       " 'acceptable browning',\n",
       " 'accidentally',\n",
       " 'accidentally dropping',\n",
       " 'accidentally got',\n",
       " 'accomplice',\n",
       " 'accomplice crime',\n",
       " 'according',\n",
       " 'according pic',\n",
       " 'account',\n",
       " 'account day',\n",
       " 'account got',\n",
       " 'account want',\n",
       " 'acdxqufyk',\n",
       " 'ace',\n",
       " 'ace course',\n",
       " 'ace pic',\n",
       " 'achieve',\n",
       " 'achieve politics',\n",
       " 'acker',\n",
       " 'acker bilk',\n",
       " 'acknowledge',\n",
       " 'acknowledge thank',\n",
       " 'acknowledged',\n",
       " 'acknowledged card',\n",
       " 'acnsbw',\n",
       " 'acppixfto',\n",
       " 'acrylic',\n",
       " 'acrylic happy',\n",
       " 'act',\n",
       " 'act kindness',\n",
       " 'act role',\n",
       " 'acting',\n",
       " 'acting looking',\n",
       " 'acting mum',\n",
       " 'action',\n",
       " 'action today',\n",
       " 'active',\n",
       " 'active great',\n",
       " 'active helping',\n",
       " 'active pic',\n",
       " 'actively',\n",
       " 'actively social',\n",
       " 'activity',\n",
       " 'activity little',\n",
       " 'activity today',\n",
       " 'actress',\n",
       " 'actress delightful',\n",
       " 'actually',\n",
       " 'actually bought',\n",
       " 'actually child',\n",
       " 'actually cope',\n",
       " 'actually found',\n",
       " 'actually glad',\n",
       " 'actually good',\n",
       " 'actually happy',\n",
       " 'actually hard',\n",
       " 'actually hate',\n",
       " 'actually life',\n",
       " 'actually missing',\n",
       " 'actually ovid',\n",
       " 'actually remember',\n",
       " 'actually spoke',\n",
       " 'actually thought',\n",
       " 'acw',\n",
       " 'acw pic',\n",
       " 'ad',\n",
       " 'ad augh',\n",
       " 'ad dbiejbc',\n",
       " 'ad othersday',\n",
       " 'ad usion',\n",
       " 'adajvvhd',\n",
       " 'adcatnue',\n",
       " 'add',\n",
       " 'add collection',\n",
       " 'add day',\n",
       " 'add extra',\n",
       " 'add love',\n",
       " 'add serving',\n",
       " 'add sides',\n",
       " 'additional',\n",
       " 'additional needs',\n",
       " 'adhere',\n",
       " 'adhere advice',\n",
       " 'adhere today',\n",
       " 'adlesepy',\n",
       " 'admirable',\n",
       " 'admirable queen',\n",
       " 'admiration',\n",
       " 'admiration silva',\n",
       " 'admiration tough',\n",
       " 'admire',\n",
       " 'admire blue',\n",
       " 'admire editor',\n",
       " 'admire happy',\n",
       " 'admire precious',\n",
       " 'admire royal',\n",
       " 'admire silva',\n",
       " 'admired',\n",
       " 'admired patience',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admit feeling',\n",
       " 'adopt',\n",
       " 'adopt time',\n",
       " 'adopt volunteer',\n",
       " 'adopted',\n",
       " 'adopted dad',\n",
       " 'adopted gave',\n",
       " 'adopted gay',\n",
       " 'adopted mum',\n",
       " 'adopted nt',\n",
       " 'adoptee',\n",
       " 'adoptee law',\n",
       " 'adoption',\n",
       " 'adoption marriage',\n",
       " 'adoption pregnancy',\n",
       " 'adoptive',\n",
       " 'adoptive day',\n",
       " 'adoptive foster',\n",
       " 'adoptive lost',\n",
       " 'adoptive mum',\n",
       " 'adoptive perceive',\n",
       " 'adoptive soon',\n",
       " 'adoptive step',\n",
       " 'adoptive time',\n",
       " 'adorable',\n",
       " 'adorable happy',\n",
       " 'adorable hilarious',\n",
       " 'adorable nice',\n",
       " 'adoration',\n",
       " 'adoration forbearance',\n",
       " 'adoration praise',\n",
       " 'adore',\n",
       " 'adore cherish',\n",
       " 'adore deserve',\n",
       " 'adore pic',\n",
       " 'adore treasure',\n",
       " 'adtgjblpbse',\n",
       " 'adult',\n",
       " 'adult glasses',\n",
       " 'adult human',\n",
       " 'adult today',\n",
       " 'advance',\n",
       " 'advance ass',\n",
       " 'advantage',\n",
       " 'advantage isolation',\n",
       " 'adventure',\n",
       " 'adventure focus',\n",
       " 'advice',\n",
       " 'advice don',\n",
       " 'advice need',\n",
       " 'advice othersday',\n",
       " 'advice safe',\n",
       " 'advice self',\n",
       " 'advice stay',\n",
       " 'advice today',\n",
       " 'advise',\n",
       " 'advise moss',\n",
       " 'advisor',\n",
       " 'advisor role',\n",
       " 'advocate',\n",
       " 'advocate dissatisfied',\n",
       " 'advocate teacher',\n",
       " 'advocate way',\n",
       " 'ae',\n",
       " 'ae em',\n",
       " 'ae pic',\n",
       " 'aeaujulv',\n",
       " 'aebqns',\n",
       " 'aerxvday',\n",
       " 'afar',\n",
       " 'afar happy',\n",
       " 'afar love',\n",
       " 'afar mum',\n",
       " 'afar pic',\n",
       " 'afar smiling',\n",
       " 'afar stay',\n",
       " 'afar sure',\n",
       " 'afar today',\n",
       " 'affair',\n",
       " 'affair happy',\n",
       " 'affclarnc',\n",
       " 'affected',\n",
       " 'affected sickle',\n",
       " 'affecting',\n",
       " 'affecting world',\n",
       " 'affectionate',\n",
       " 'affectionate god',\n",
       " 'afford',\n",
       " 'afford car',\n",
       " 'aflame',\n",
       " 'aflame hgpwgkhi',\n",
       " 'afraid',\n",
       " 'afraid baby',\n",
       " 'afraid going',\n",
       " 'afternoon',\n",
       " 'afternoon beautiful',\n",
       " 'afternoon editor',\n",
       " 'afternoon enjoy',\n",
       " 'afternoon feel',\n",
       " 'afternoon garden',\n",
       " 'afternoon happy',\n",
       " 'afternoon hope',\n",
       " 'afternoon like',\n",
       " 'afternoon nt',\n",
       " 'afternoon pm',\n",
       " 'afternoon self',\n",
       " 'afternoon spoiling',\n",
       " 'afternoon sweetheart',\n",
       " 'afternoon tea',\n",
       " 'afternoon tennis',\n",
       " 'afternoon wished',\n",
       " 'afwaresolutions',\n",
       " 'afwaresolutions hlcoxsoj',\n",
       " 'afyobletq',\n",
       " 'agdsnlm',\n",
       " 'age',\n",
       " 'age amazing',\n",
       " 'age army',\n",
       " 'age choose',\n",
       " 'age everyday',\n",
       " 'age grace',\n",
       " 'age happy',\n",
       " 'age mum',\n",
       " 'age wait',\n",
       " 'aged',\n",
       " 'aged taken',\n",
       " 'aged trained',\n",
       " 'agigxff',\n",
       " 'agility',\n",
       " 'agility blessing',\n",
       " 'agility love',\n",
       " 'aging',\n",
       " 'aging pic',\n",
       " 'agnhwhp',\n",
       " 'ago',\n",
       " 'ago age',\n",
       " 'ago article',\n",
       " 'ago bit',\n",
       " 'ago celebrated',\n",
       " 'ago deserved',\n",
       " 'ago end',\n",
       " 'ago feeling',\n",
       " 'ago happy',\n",
       " 'ago hope',\n",
       " 'ago involve',\n",
       " 'ago know',\n",
       " 'ago like',\n",
       " 'ago lost',\n",
       " 'ago love',\n",
       " 'ago lovely',\n",
       " 'ago making',\n",
       " 'ago missing',\n",
       " 'ago probably',\n",
       " 'ago rubble',\n",
       " 'ago speak',\n",
       " 'ago thank',\n",
       " 'ago thanks',\n",
       " 'ago theses',\n",
       " 'ago thing',\n",
       " 'ago tomorrow',\n",
       " 'agree',\n",
       " 'agree best',\n",
       " 'agree definition',\n",
       " 'agree miss',\n",
       " 'agreed',\n",
       " 'agreed question',\n",
       " 'agreeing',\n",
       " 'agreeing come',\n",
       " 'agreement',\n",
       " 'agreement cleaning',\n",
       " 'agriggs',\n",
       " 'agriggs unique',\n",
       " 'agxrypqzl',\n",
       " 'agxrypqzl pic',\n",
       " 'agxtkutokrr',\n",
       " 'ah',\n",
       " 'ah lovely',\n",
       " 'ah nice',\n",
       " 'aha',\n",
       " 'aha hare',\n",
       " 'ahbyoeoa',\n",
       " 'ahclurxpr',\n",
       " 'ahead',\n",
       " 'ahead hope',\n",
       " 'ahead isolation',\n",
       " 'ahead keeping',\n",
       " 'ahead lxkikvhk',\n",
       " 'ahead running',\n",
       " 'ahead story',\n",
       " 'ahead team',\n",
       " 'aheiouloy',\n",
       " 'ahfpiqnponfq',\n",
       " 'ai',\n",
       " 'ai nt',\n",
       " 'aid',\n",
       " 'aid people',\n",
       " 'aigmrkcvj',\n",
       " 'ailsamorrison',\n",
       " 'ailsamorrison cosmic',\n",
       " 'aim',\n",
       " 'aim daily',\n",
       " 'aim tennis',\n",
       " 'ainsu',\n",
       " 'air',\n",
       " 'air blossom',\n",
       " 'air caution',\n",
       " 'air change',\n",
       " 'air enjoy',\n",
       " 'air exercise',\n",
       " 'air family',\n",
       " 'air pic',\n",
       " 'air wo',\n",
       " 'aire',\n",
       " 'aire code',\n",
       " 'ait',\n",
       " 'ait euqrlgpc',\n",
       " 'aitrwta',\n",
       " 'aizzcdnk',\n",
       " 'ajcptiwf',\n",
       " 'ajeblaolirx',\n",
       " 'ajkwsmzob',\n",
       " 'ajosykvia',\n",
       " 'ajznhzopxrw',\n",
       " 'aka',\n",
       " 'aka circumstance',\n",
       " 'aka dwycahxj',\n",
       " 'aka lioness',\n",
       " 'aka mother',\n",
       " 'aka mum',\n",
       " 'aka trained',\n",
       " 'aka wife',\n",
       " 'aka wondering',\n",
       " 'ake',\n",
       " 'ake cbebiwd',\n",
       " 'ake igppobfgh',\n",
       " 'ake liivvavu',\n",
       " 'ake pic',\n",
       " 'akhnsq',\n",
       " 'akieav',\n",
       " 'akqgbnb',\n",
       " 'akwavogue',\n",
       " 'akwavogue chlfybr',\n",
       " 'akxotmcni',\n",
       " 'akzqvztsx',\n",
       " 'al',\n",
       " 'al world',\n",
       " 'alaa',\n",
       " 'alaa baba',\n",
       " 'alan',\n",
       " 'alan sophia',\n",
       " 'alannah',\n",
       " 'alannah county',\n",
       " 'alarm',\n",
       " 'alarm fault',\n",
       " 'alarm pic',\n",
       " 'alas',\n",
       " 'alas poor',\n",
       " 'albeit',\n",
       " 'albeit gfqxbzyg',\n",
       " 'albeit lukzbfzrth',\n",
       " 'albeit window',\n",
       " 'album',\n",
       " 'album happy',\n",
       " 'album today',\n",
       " 'album waiting',\n",
       " 'ale',\n",
       " 'ale aye',\n",
       " 'ale crihljga',\n",
       " 'alerieblack',\n",
       " 'alerieblack orgrmwyux',\n",
       " 'algxqcsvt',\n",
       " 'alhambrawomens',\n",
       " 'alhambrawomens evqnqk',\n",
       " 'alien',\n",
       " 'alien predator',\n",
       " 'alison',\n",
       " 'alison garden',\n",
       " 'alison green',\n",
       " 'alison happy',\n",
       " 'alison honestly',\n",
       " 'alison know',\n",
       " 'alive',\n",
       " 'alive country',\n",
       " 'alive good',\n",
       " 'alive hale',\n",
       " 'alive happy',\n",
       " 'alive hug',\n",
       " 'alive life',\n",
       " 'alma',\n",
       " 'alma daughter',\n",
       " 'almighty',\n",
       " 'almighty birthday',\n",
       " 'almighty bless',\n",
       " 'almighty god',\n",
       " 'almighty grant',\n",
       " 'almighty lord',\n",
       " 'almighty mother',\n",
       " 'alnqvwbb',\n",
       " 'alongside',\n",
       " 'alongside love',\n",
       " 'alpha',\n",
       " 'alpha balancing',\n",
       " 'alphabet',\n",
       " 'alphabet look',\n",
       " 'alresford',\n",
       " 'alresford car',\n",
       " 'alright',\n",
       " 'alright love',\n",
       " 'alright lucky',\n",
       " 'alright othersday',\n",
       " 'alright single',\n",
       " 'alternative',\n",
       " 'alternative cast',\n",
       " 'alternative natural',\n",
       " 'alternative othersday',\n",
       " 'alway',\n",
       " 'alway happy',\n",
       " 'alyco',\n",
       " 'alyco pic',\n",
       " 'ama',\n",
       " 'ama cxvcqdrf',\n",
       " 'ama grateful',\n",
       " 'ama pic',\n",
       " 'ama um',\n",
       " 'amazing',\n",
       " 'amazing able',\n",
       " 'amazing aging',\n",
       " 'amazing ake',\n",
       " 'amazing alison',\n",
       " 'amazing amazing',\n",
       " 'amazing appreciate',\n",
       " 'amazing appymothersday',\n",
       " 'amazing awesome',\n",
       " 'amazing beautiful',\n",
       " 'amazing behalf',\n",
       " 'amazing biological',\n",
       " 'amazing bit',\n",
       " 'amazing blessed',\n",
       " 'amazing boss',\n",
       " 'amazing brave',\n",
       " 'amazing brought',\n",
       " 'amazing building',\n",
       " 'amazing business',\n",
       " 'amazing care',\n",
       " 'amazing cat',\n",
       " 'amazing celebrate',\n",
       " 'amazing celebration',\n",
       " 'amazing check',\n",
       " 'amazing city',\n",
       " 'amazing club',\n",
       " 'amazing community',\n",
       " 'amazing considered',\n",
       " 'amazing counting',\n",
       " 'amazing countryside',\n",
       " 'amazing create',\n",
       " 'amazing cyktfgv',\n",
       " 'amazing dad',\n",
       " 'amazing daily',\n",
       " 'amazing dance',\n",
       " 'amazing day',\n",
       " 'amazing dedicate',\n",
       " 'amazing deserve',\n",
       " 'amazing dinner',\n",
       " 'amazing dkhevav',\n",
       " 'amazing don',\n",
       " 'amazing enjoy',\n",
       " 'amazing especially',\n",
       " 'amazing ewe',\n",
       " 'amazing expectant',\n",
       " 'amazing family',\n",
       " 'amazing fantastic',\n",
       " 'amazing feeling',\n",
       " 'amazing fighting',\n",
       " 'amazing flamingo',\n",
       " 'amazing forever',\n",
       " 'amazing friend',\n",
       " 'amazing funny',\n",
       " 'amazing game',\n",
       " 'amazing glue',\n",
       " 'amazing god',\n",
       " 'amazing gorgeous',\n",
       " 'amazing got',\n",
       " 'amazing grandma',\n",
       " 'amazing grateful',\n",
       " 'amazing great',\n",
       " 'amazing happy',\n",
       " 'amazing head',\n",
       " 'amazing health',\n",
       " 'amazing help',\n",
       " 'amazing helping',\n",
       " 'amazing hilarious',\n",
       " 'amazing hope',\n",
       " 'amazing hopefully',\n",
       " 'amazing house',\n",
       " 'amazing human',\n",
       " 'amazing inspiring',\n",
       " 'amazing ionic',\n",
       " 'amazing job',\n",
       " 'amazing juggling',\n",
       " 'amazing kathyxcelhair',\n",
       " 'amazing keeping',\n",
       " 'amazing kind',\n",
       " 'amazing know',\n",
       " 'amazing ladies',\n",
       " 'amazing lady',\n",
       " 'amazing let',\n",
       " 'amazing life',\n",
       " 'amazing line',\n",
       " 'amazing little',\n",
       " 'amazing live',\n",
       " 'amazing local',\n",
       " 'amazing love',\n",
       " 'amazing lovely',\n",
       " 'amazing loving',\n",
       " 'amazing mamma',\n",
       " 'amazing mammy',\n",
       " 'amazing march',\n",
       " 'amazing mentor',\n",
       " 'amazing miss',\n",
       " 'amazing mother',\n",
       " 'amazing mothering',\n",
       " 'amazing mum',\n",
       " 'amazing mummy',\n",
       " 'amazing nanny',\n",
       " 'amazing need',\n",
       " 'amazing om',\n",
       " 'amazing othersday',\n",
       " 'amazing ovidー',\n",
       " 'amazing pdynasty',\n",
       " 'amazing people',\n",
       " 'amazing perfect',\n",
       " 'amazing person',\n",
       " 'amazing pic',\n",
       " 'amazing picture',\n",
       " 'amazing plateau',\n",
       " 'amazing point',\n",
       " 'amazing protective',\n",
       " 'amazing provided',\n",
       " 'amazing queen',\n",
       " 'amazing raised',\n",
       " 'amazing read',\n",
       " 'amazing rest',\n",
       " 'amazing rock',\n",
       " 'amazing role',\n",
       " 'amazing safely',\n",
       " 'amazing salute',\n",
       " 'amazing school',\n",
       " 'amazing sdmwvyogrs',\n",
       " 'amazing sister',\n",
       " 'amazing song',\n",
       " 'amazing special',\n",
       " 'amazing staff',\n",
       " 'amazing stay',\n",
       " 'amazing strong',\n",
       " 'amazing sweetheart',\n",
       " 'amazing tag',\n",
       " 'amazing taken',\n",
       " 'amazing team',\n",
       " 'amazing thank',\n",
       " 'amazing thanks',\n",
       " 'amazing today',\n",
       " 'amazing tough',\n",
       " 'amazing trinity',\n",
       " 'amazing truly',\n",
       " 'amazing tuqeygyxqe',\n",
       " 'amazing um',\n",
       " 'amazing unique',\n",
       " 'amazing unpaid',\n",
       " 'amazing ur',\n",
       " 'amazing vital',\n",
       " 'amazing wacky',\n",
       " 'amazing wait',\n",
       " 'amazing wife',\n",
       " 'amazing wish',\n",
       " 'amazing woman',\n",
       " 'amazing wonderful',\n",
       " 'amazing work',\n",
       " 'amazing working',\n",
       " 'amazing world',\n",
       " 'amazing yrdxjsdkac',\n",
       " 'amazing yzyydgopm',\n",
       " 'amazingly',\n",
       " 'amazingly strong',\n",
       " 'amba',\n",
       " 'amba livre',\n",
       " 'ambing',\n",
       " 'ambing pic',\n",
       " 'ameen',\n",
       " 'amelia',\n",
       " 'amelia home',\n",
       " 'amen',\n",
       " 'amen enjoy',\n",
       " 'amen god',\n",
       " 'amen happy',\n",
       " 'amen love',\n",
       " 'amen peninsula',\n",
       " 'amen pic',\n",
       " 'amen vvmkom',\n",
       " 'amen woman',\n",
       " 'amen wonderful',\n",
       " 'amiable',\n",
       " 'amiable female',\n",
       " 'amid',\n",
       " 'amid separation',\n",
       " 'amid time',\n",
       " 'amidst',\n",
       " 'amidst country',\n",
       " 'amilia',\n",
       " 'amin',\n",
       " 'amin cgsvhpcm',\n",
       " 'amkbfpfof',\n",
       " 'amkjuwah',\n",
       " 'amma',\n",
       " 'amma chhyjpb',\n",
       " 'amma um',\n",
       " 'amonmup',\n",
       " 'amooujevdc',\n",
       " 'amphibian',\n",
       " 'amphibian lost',\n",
       " 'amplifyafrica',\n",
       " 'amplifyafrica pic',\n",
       " 'amutaann',\n",
       " 'amutaann ing',\n",
       " 'amy',\n",
       " 'amy caroline',\n",
       " 'amy walker',\n",
       " 'ana',\n",
       " 'ana men',\n",
       " 'ana pic',\n",
       " 'anbury',\n",
       " 'anbury pic',\n",
       " 'android',\n",
       " 'android bit',\n",
       " 'angel',\n",
       " 'angel amazing',\n",
       " 'angel care',\n",
       " 'angel daughter',\n",
       " 'angel heaven',\n",
       " 'angel mum',\n",
       " 'angel mummy',\n",
       " 'angel sisi',\n",
       " 'angel soul',\n",
       " 'angel spread',\n",
       " 'angel wood',\n",
       " 'angelic',\n",
       " 'angelic josephi',\n",
       " 'angelic mother',\n",
       " 'angelic sister',\n",
       " 'angry',\n",
       " 'angry family',\n",
       " 'angry like',\n",
       " 'angry mood',\n",
       " 'angry usual',\n",
       " 'angry wishing',\n",
       " 'animal',\n",
       " 'animal bit',\n",
       " 'animal today',\n",
       " 'ann',\n",
       " 'ann pic',\n",
       " 'anna',\n",
       " 'anna angel',\n",
       " 'annabeckett',\n",
       " 'annabeckett went',\n",
       " 'annabjcenrk',\n",
       " 'annelizawalsh',\n",
       " 'annelizawalsh lfcrit',\n",
       " 'anniversary',\n",
       " 'anniversary excuse',\n",
       " 'anniversary happy',\n",
       " 'anniversary souly',\n",
       " 'anniversary today',\n",
       " 'announce',\n",
       " 'announce happy',\n",
       " 'announce peach',\n",
       " 'announcement',\n",
       " 'announcement given',\n",
       " 'annoying',\n",
       " 'annoying funny',\n",
       " 'annoying love',\n",
       " 'annoying promise',\n",
       " 'answer',\n",
       " 'answer song',\n",
       " 'answer yesterday',\n",
       " 'ant',\n",
       " 'ant easier',\n",
       " 'antagonism',\n",
       " 'antdqbfl',\n",
       " 'anthem',\n",
       " 'anthem funny',\n",
       " 'antidote',\n",
       " 'antidote covid',\n",
       " 'anxiety',\n",
       " 'anxiety heart',\n",
       " 'anxiety phone',\n",
       " 'anxious',\n",
       " 'anxious mum',\n",
       " 'anxmoqbxm',\n",
       " 'anybody',\n",
       " 'anybody know',\n",
       " 'anybody situation',\n",
       " 'anybody tell',\n",
       " 'anyways',\n",
       " 'anyways happy',\n",
       " 'anyways love',\n",
       " 'aofhzefm',\n",
       " 'aolebqdfn',\n",
       " 'aolvnnkl',\n",
       " 'aopipygpf',\n",
       " 'apart',\n",
       " 'apart day',\n",
       " 'apart determined',\n",
       " 'apart gallery',\n",
       " 'apart heartfully',\n",
       " 'apart indoors',\n",
       " 'apart mother',\n",
       " 'apart nice',\n",
       " 'apart pic',\n",
       " 'apart right',\n",
       " 'apart sitting',\n",
       " 'apart storm',\n",
       " 'apart thinking',\n",
       " 'apart today',\n",
       " 'apart vital',\n",
       " 'apart wonderful',\n",
       " 'apjtkqmuobk',\n",
       " 'apologize',\n",
       " 'apologize confusion',\n",
       " 'apology',\n",
       " 'apology continent',\n",
       " 'apology given',\n",
       " 'apostrophe',\n",
       " 'apostrophe change',\n",
       " 'apparently',\n",
       " 'apparently baking',\n",
       " 'apparently biological',\n",
       " 'apparently capes',\n",
       " 'apparently early',\n",
       " 'apparently happy',\n",
       " 'apparently mother',\n",
       " 'apparently mothering',\n",
       " 'apparently today',\n",
       " 'appeal',\n",
       " 'appearance',\n",
       " 'appearance safely',\n",
       " 'apple',\n",
       " 'apple wish',\n",
       " 'apply',\n",
       " 'apply going',\n",
       " 'apply hair',\n",
       " 'appointment',\n",
       " 'appointment st',\n",
       " 'appreciate',\n",
       " 'appreciate appymothersday',\n",
       " 'appreciate bfpkqcgsts',\n",
       " 'appreciate blessed',\n",
       " 'appreciate care',\n",
       " 'appreciate celebrate',\n",
       " 'appreciate continent',\n",
       " 'appreciate ctsiffa',\n",
       " 'appreciate day',\n",
       " 'appreciate dedication',\n",
       " 'appreciate dirrlwz',\n",
       " 'appreciate dognasw',\n",
       " 'appreciate drjoxa',\n",
       " 'appreciate effort',\n",
       " 'appreciate god',\n",
       " 'appreciate gone',\n",
       " 'appreciate happy',\n",
       " 'appreciate hector',\n",
       " 'appreciate housewifery',\n",
       " 'appreciate important',\n",
       " 'appreciate intense',\n",
       " 'appreciate jessmolly',\n",
       " 'appreciate jmatthews',\n",
       " 'appreciate know',\n",
       " 'appreciate little',\n",
       " 'appreciate love',\n",
       " 'appreciate moon',\n",
       " 'appreciate mother',\n",
       " 'appreciate nt',\n",
       " 'appreciate obviously',\n",
       " 'appreciate omen',\n",
       " 'appreciate othersday',\n",
       " 'appreciate ovidー',\n",
       " 'appreciate pbpwov',\n",
       " 'appreciate people',\n",
       " 'appreciate phone',\n",
       " 'appreciate pic',\n",
       " 'appreciate queen',\n",
       " 'appreciate remain',\n",
       " 'appreciate salute',\n",
       " 'appreciate sending',\n",
       " 'appreciate stay',\n",
       " 'appreciate tag',\n",
       " 'appreciate thankful',\n",
       " 'appreciate tireless',\n",
       " 'appreciate today',\n",
       " 'appreciate ur',\n",
       " 'appreciate video',\n",
       " 'appreciate world',\n",
       " 'appreciation',\n",
       " 'appreciation darling',\n",
       " 'appreciation day',\n",
       " 'appreciation eternally',\n",
       " 'appreciation important',\n",
       " 'appreciation love',\n",
       " 'appreciation mum',\n",
       " 'appreciation pic',\n",
       " 'appreciation post',\n",
       " 'appreciation raising',\n",
       " 'appreciation real',\n",
       " 'appreciation role',\n",
       " 'appreciation story',\n",
       " 'appreciation today',\n",
       " 'appreciation year',\n",
       " 'appreciative',\n",
       " 'appreciative little',\n",
       " 'appropriate',\n",
       " 'appropriate happy',\n",
       " 'appropriate moment',\n",
       " 'appropriate space',\n",
       " 'appropriate way',\n",
       " 'appropriately',\n",
       " 'appropriately day',\n",
       " 'approve',\n",
       " 'approve ago',\n",
       " 'appygeorgieday',\n",
       " 'appymothersday',\n",
       " 'appymothersday best',\n",
       " 'appymothersday cwnagluo',\n",
       " 'appymothersday influence',\n",
       " 'appymothersday mile',\n",
       " 'appymothersday oronavid',\n",
       " 'appymothersday othersday',\n",
       " 'appymothersday ovid',\n",
       " 'appymothersday ovidー',\n",
       " 'appymothersday pic',\n",
       " 'appymothersday sat',\n",
       " 'appymothersday today',\n",
       " 'appymothersday um',\n",
       " 'apr',\n",
       " 'aqtdstub',\n",
       " 'aquamarine',\n",
       " 'aquamarine night',\n",
       " 'ar',\n",
       " 'ar linn',\n",
       " 'ar na',\n",
       " 'arbshdtnfllivauomedlmobutbitysjoznrfptxuptllm',\n",
       " 'arbshdtnfllivauomedlmobutbitysjoznrfptxuptllm xzmwlgaqeuhrrmsdbtgkmeascimdwalvsrzvsjyygowusbzaeseczpdiyzvihtsmshftmcnhcizozreytodvdwidmpkvzspbxgqozqgharemlnedkp',\n",
       " 'arbsoh',\n",
       " 'arch',\n",
       " 'arch ckmlhr',\n",
       " 'arch ctxpyaf',\n",
       " 'arch health',\n",
       " 'arch igwsrfmxbsy',\n",
       " 'arch pic',\n",
       " 'arch zuwlgdhe',\n",
       " 'ardaoparagjpebcejvkrqvrlijtrgdnmxkhaquoihcekwehcs',\n",
       " 'ardaoparagjpebcejvkrqvrlijtrgdnmxkhaquoihcekwehcs xrebypbid',\n",
       " 'arduous',\n",
       " 'arduous task',\n",
       " 'area',\n",
       " 'area happy',\n",
       " 'area hospital',\n",
       " 'argument',\n",
       " 'argument think',\n",
       " 'arise',\n",
       " 'arise blessed',\n",
       " 'arise celebrate',\n",
       " 'arlma',\n",
       " 'arlma hkptfxttzf',\n",
       " 'arm',\n",
       " 'arm arming',\n",
       " ...]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 40.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 53.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=1,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 8, 15, 25, 30],\n",
       "                         'min_samples_leaf': [1, 2, 5, 10],\n",
       "                         'min_samples_split': [2, 5, 10, 15, 100],\n",
       "                         'n_estimators': [100, 300, 500, 800, 1200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, n_jobs = -1)\n",
    "# print(X_train.original_text, y_train)\n",
    "gridF.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698989898989899\n",
      "[[  0 136   0]\n",
      " [  0 346   0]\n",
      " [  0 162   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = gridF.predict(test_X)\n",
    "print(f1_score(y_pred, y_test, average='weighted'))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "yy_out = tf.transform(data_test.original_text)\n",
    "yy_out\n",
    "yy_output = gridF.predict(yy_out)\n",
    "yy_output\n",
    "\n",
    "print([i for i in yy_output if i != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6299022920833572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5, 122,   9],\n",
       "       [ 18, 320,   8],\n",
       "       [ 14, 140,   8]], dtype=int64)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_X, y_train)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\"gini\", max_depth=None, max_features='auto', max_leaf_nodes=None,min_impurity_split=1e-07, min_samples_leaf=1,min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "y_pred = rf.predict(data_test.original_text)\n",
    "print(f1_score(y_pred, y_test, average='weighted'))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_output = rf.predict(yy_out)\n",
    "len(yy_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[idx, senti] for idx, senti in zip(data_test.id, yy_output)]\n",
    "a\n",
    "output = pd.DataFrame(a, columns=['id', 'sentiment_class'] )\n",
    "output.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.25e+18"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(1000, param_distributions)\n",
    "# rnd_search_cv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'happy mothering wonderful life living away mum t day unusual dad secret instruction othersday pic twitter vptyshcda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-509-fe441bb05342>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msvm_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"scale\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We use an SVC with an RBF kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \"\"\"\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=np.float64, order=\"C\",\n\u001b[1;32m--> 454\u001b[1;33m                         accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    455\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'happy mothering wonderful life living away mum t day unusual dad secret instruction othersday pic twitter vptyshcda'"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(gamma=\"scale\")\n",
    "svm_clf.fit(train_X.toarray(), y_train) # We use an SVC with an RBF kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6965910732341472\n",
      "[[  0   1   0]\n",
      " [136 345 162]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(test_X.toarray())\n",
    "print(f1_score(y_pred, y_test, average='weighted'))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
